{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evalsingletask.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sshetty78/Face-Mask-detection/blob/main/Evalsingletask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7ljljKPuCU9"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "os.chdir(\"/content/drive/MyDrive/ColabFolder/\")\n",
        "import BKNetStyle\n",
        "from const import *\n",
        "import time\n",
        "import math\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from copy import copy, deepcopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iehZ0wJwalY"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4znSgXmuWf6",
        "outputId": "5615e401-82f8-430c-d08e-af26d667edd2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeCBeLq7v4eQ"
      },
      "source": [
        "''' PREPARE DATA '''\n",
        "''' PREPARE DATA '''\n",
        "#smile_train, smile_test = CNN2Head_input.getSmileImage()\n",
        "#gender_train, gender_test = CNN2Head_input.getGenderImage()\n",
        "#age_train, age_test = CNN2Head_input.getAgeImage()\n",
        "\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "def normalise(x):\n",
        "    return (x-128)/255.0\n",
        "\n",
        "def load_mini_dataset(index):\n",
        "    \n",
        "    label_to_int = {\"Mask\":0,\"Mask_Mouth_Chin\":1,\"Mask_Chin\":2,\"Mask_Nose_Mouth\":3}\n",
        "    int_to_label = {0:\"Mask\",1:\"Mask_Mouth_Chin\",2:\"Mask_Chin\",3:\"Mask_Nose_Mouth\"}\n",
        "\n",
        "\n",
        "    filename_prefix = '/content/drive/MyDrive/ColabFolder/test_data/test_part'#/content/drive/MyDrive/GT/BigDataProject/train_part0.npy\n",
        "    filename = filename_prefix + str(index) + \".npy\"\n",
        "                \n",
        "    X = np.load(filename, allow_pickle=True)\n",
        "  \n",
        "    x_mini_data =[]\n",
        "    y_mini_data=[]\n",
        "    mask = []    \n",
        "    mask_prim = 0.0\n",
        "    mask_sec = 1.0\n",
        "\n",
        "    for sample in X:\n",
        "        \n",
        "        norm_x_prim = normalise(sample[0]) # todo check output once\n",
        "\n",
        "        label_int = sample[1]\n",
        "        y_prim = [0] * NUM_CLASSES\n",
        "\n",
        "        if int_to_label[label_int]==\"Mask\":\n",
        "          x_mini_data.append(norm_x_prim)\n",
        "          y_prim[label_int] = 1 # y_ = [1,0,0]\n",
        "          y_mini_data.append(y_prim)\n",
        "          mask.append(mask_prim)\n",
        "          \n",
        "        else:\n",
        "          x_mini_data.append(norm_x_prim)\n",
        "          y_prim[1] = 1 # y_ = [0,1,0]\n",
        "          y_mini_data.append(y_prim)\n",
        "          mask.append(mask_prim)\n",
        "\n",
        "          # create a copy for x & y, add same x and new y for secondary task\n",
        "          norm_x_sec = deepcopy(norm_x_prim)\n",
        "          y_sec = [0] * NUM_CLASSES\n",
        "          y_sec[label_int-1] = 1\n",
        "          \n",
        "          x_mini_data.append(norm_x_sec)\n",
        "          y_mini_data.append(y_sec)\n",
        "\n",
        "          mask.append(mask_sec)\n",
        "          \n",
        "\n",
        "    return x_mini_data, y_mini_data,mask\n",
        "\n",
        "\n",
        "def create_random_mini_batches(X, Y, mask_mini, mini_batch_size = 32):\n",
        "    \n",
        "    m = len(X) #.shape[0] # number of training examples\n",
        "    mini_batches = []\n",
        "    \n",
        "    #print(\"Shape of X =\", X.shape)\n",
        "    #print(\"Shape of Y =\", Y.shape)\n",
        "    \n",
        "    #Reshaping to convert Y to a 2D array from a rank one array\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    Z = np.array(mask_mini)\n",
        "    #Y = Y.reshape(Y.shape[0], 1) #todo Samarth check y shape\n",
        "    # Y = one_hot(Y, NUM_CLASSES)\n",
        "    #print(Y)\n",
        "\n",
        "    #Shuffle the data in each of the mini batch\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation, :]\n",
        "    shuffled_Y = Y[permutation, :]\n",
        "    shuffled_Z = Z[permutation]\n",
        "    \n",
        "    n_mini_batches = math.ceil(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(n_mini_batches):\n",
        "        \n",
        "        start_pos = k * mini_batch_size\n",
        "        end_pos = min(start_pos + mini_batch_size, m)\n",
        "        \n",
        "        mini_batch_X = shuffled_X[start_pos : end_pos, :]\n",
        "        mini_batch_Y = shuffled_Y[start_pos : end_pos, :]\n",
        "        mini_batch_Z = shuffled_Z[start_pos : end_pos]\n",
        "        \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y,mini_batch_Z)\n",
        "        mini_batches.append(mini_batch)\n",
        "\n",
        "    return mini_batches\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8Jmx7rPwGol",
        "outputId": "22f5d167-67b8-46eb-d71b-bd7f1765ec00"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def one_hot(indices, num_classes):\n",
        "    tmp = np.zeros((indices.shape[0], num_classes), dtype=np.float32)\n",
        "    tmp[np.arange(indices.shape[0]), indices] = 1.0\n",
        "    return tmp\n",
        "\n",
        "\n",
        "a = np.array([1, 0, 1, 2])\n",
        "# print(a)\n",
        "a = a.reshape(a.shape[0], 1)\n",
        "# print(a)\n",
        "# print(a.squeeze())\n",
        "one_hot(a.squeeze(), 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c_zjXbHwSZt",
        "outputId": "7391c8c8-857b-42c8-8ab9-a36426e0dd92"
      },
      "source": [
        "sess = tf.compat.v1.InteractiveSession()\n",
        "global_step = tf.contrib.framework.get_or_create_global_step()\n",
        "\n",
        "x, y_, mask = BKNetStyle.Input()\n",
        "\n",
        "y_prim_conv, phase_train, keep_prob = BKNetStyle.BKNetModel(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-8-b19443ae1323>:2: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/drive/MyDrive/ColabFolder/BKNetStyle.py:38: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQ5AF5bZ1AwK",
        "outputId": "a4c74be2-cb26-463b-efe8-c7e5b9db4379"
      },
      "source": [
        "import BKNetStyle\n",
        "from const import *\n",
        "import os\n",
        "prim_loss, l2_loss, loss = BKNetStyle.selective_loss(y_prim_conv, y_, mask)\n",
        "# train_step = BKNetStyle.train_op(loss, global_step)\n",
        "\n",
        "prim_mask = tf.get_collection('face_mask')[0]\n",
        "# sec_mask = tf.get_collection('sec_mask')[0]\n",
        "# age_mask = tf.get_collection('age_mask')[0]\n",
        "\n",
        "y_prim = tf.get_collection('y_face')[0]\n",
        "# y_sec = tf.get_collection('y_sec')[0]\n",
        "# y_age = tf.get_collection('y_age')[0]\n",
        "\n",
        "prim_correct_prediction = tf.equal(tf.argmax(y_prim_conv, 1), tf.argmax(y_prim, 1))\n",
        "# sec_correct_prediction = tf.equal(tf.argmax(y_sec_conv, 1), tf.argmax(y_sec, 1))\n",
        "# age_correct_prediction = tf.equal(tf.argmax(y_age_conv, 1), tf.argmax(y_age, 1))\n",
        "\n",
        "prim_true_pred = tf.reduce_sum(tf.cast(prim_correct_prediction, dtype=tf.float32) * prim_mask)\n",
        "# sec_true_pred = tf.reduce_sum(tf.cast(sec_correct_prediction, dtype=tf.float32) * sec_mask)\n",
        "# age_true_pred = tf.reduce_sum(tf.cast(age_correct_prediction, dtype=tf.float32) * age_mask)\n",
        "\n",
        "\n",
        "print('Restore model')\n",
        "saver = tf.train.Saver()\n",
        "saver.restore(sess, './sample_data/' + 'model.ckpt')\n",
        "print('OK')\n",
        "\n",
        "\n",
        "prim_nb_true_pred = 0\n",
        "\n",
        "prim_nb_train = 0\n",
        "\n",
        "# print(\"Learning rate: %f\" % learning_rate.eval())\n",
        "    #added by Samarth; lazy loading of dataset and training\n",
        "n_mini_batches = 0\n",
        "epoch_cost = 0\n",
        "total_time_taken_to_load_dataset = 0\n",
        "total_time_taken_to_create_mini_batches = 0            \n",
        "tic = time.time()\n",
        "#file_indices = get_shuffled_indices(1, 26) # todo change to total part files\n",
        "file_indices = [0] #TODO - to remove\n",
        "\n",
        "mini_batch_size = BATCH_SIZE\n",
        "true_pos_count = 0\n",
        "false_pos_count = 0\n",
        "false_neg_count = 0\n",
        "true_neg_count = 0\n",
        "TP = tf.count_nonzero(tf.cast(tf.argmax(y_prim_conv, 1) * tf.argmax(y_prim, 1), dtype=tf.float32) * prim_mask, axis=0)\n",
        "FP = tf.count_nonzero(tf.cast(tf.argmax(y_prim_conv, 1) * (tf.argmax(y_prim, 1) - 1), dtype=tf.float32) * prim_mask, axis=0)\n",
        "FN = tf.count_nonzero(tf.cast((tf.argmax(y_prim_conv, 1) - 1) * tf.argmax(y_prim, 1), dtype=tf.float32) * prim_mask, axis=0)\n",
        "TN = tf.count_nonzero(tf.cast((tf.argmax(y_prim_conv, 1) - 1) * (tf.argmax(y_prim, 1) - 1), dtype=tf.float32) * prim_mask, axis=0)\n",
        "\n",
        "\n",
        "for j in file_indices:\n",
        "    \n",
        "    X_mini = None\n",
        "    Y_mini = None\n",
        "\n",
        "    lmd_tic = time.time()\n",
        "    X_mini, Y_mini, mask_mini = load_mini_dataset(j) # todo Samarth add for mask - done\n",
        "\n",
        "    lmd_toc = time.time()\n",
        "    total_time_taken_to_load_dataset += (lmd_toc-lmd_tic)\n",
        "    \n",
        "    rmb_tic = time.time()\n",
        "    mini_batches = None\n",
        "    mini_batches = create_random_mini_batches(X_mini, Y_mini, mask_mini, mini_batch_size = mini_batch_size) # todo Samarth add for mask\n",
        "    rmb_toc = time.time()\n",
        "    total_time_taken_to_create_mini_batches += (rmb_toc-rmb_tic)\n",
        "\n",
        "    for mini_batch in mini_batches:\n",
        "        batch_img = mini_batch[0] \n",
        "        batch_label = mini_batch[1]\n",
        "        batch_mask = mini_batch[2]\n",
        "\n",
        "        # (batch_img, batch_label) = mini_batch\n",
        "        curr_batch_size = batch_img.shape[0]\n",
        "        # batch_mask = np.zeros(curr_batch_size) #todo comment Samarth - done\n",
        "\n",
        "        for i in range(curr_batch_size):\n",
        "            if batch_mask[i] == 0.0:\n",
        "                prim_nb_train += 1\n",
        "            # else:\n",
        "                # if batch_mask[i] == 1.0:\n",
        "                    # sec_nb_train += 1                        \n",
        "                # else:\n",
        "                #     age_nb_train += 1\n",
        "        # print(\"get shape batch img 1\")\n",
        "        # print(tf.shape(batch_img))\n",
        "        # batch_img = CNN2Head_input.augmentation(batch_img, 48)\n",
        "        # print(tf.shape(batch_img))\n",
        "        # print(tf.shape(batch_label))\n",
        "\n",
        "        # ttl, pml, l2l, _ = sess.run([loss, prim_loss, l2_loss, train_step],\n",
        "        #                                       feed_dict={x: batch_img, y_: batch_label, mask: batch_mask,\n",
        "        #                                                 phase_train: True, keep_prob: 0.5})\n",
        "\n",
        "        prim_nb_true_pred += sess.run(prim_true_pred, feed_dict={x: batch_img, y_: batch_label, mask: batch_mask,\n",
        "                                                                  phase_train: False,\n",
        "                                                                  keep_prob: 0.5})\n",
        "\n",
        "        true_pos, false_pos, false_neg, true_neg = sess.run([TP, FP, FN, TN], feed_dict={x: batch_img, y_: batch_label, mask: batch_mask,\n",
        "                                                  phase_train: False,\n",
        "                                                  keep_prob: 0.5})\n",
        "        true_pos_count += true_pos\n",
        "        false_pos_count += false_pos\n",
        "        false_neg_count += false_neg\n",
        "        true_neg_count += true_neg\n",
        "\n",
        "\n",
        "prim_train_accuracy = prim_nb_true_pred * 1.0 / prim_nb_train\n",
        "\n",
        "precision_class0 = true_pos_count / (true_pos_count + false_pos_count)\n",
        "recall_class0 = true_pos_count / (true_pos_count + false_neg_count)\n",
        "f1_class0 = 2 * precision_class0 * recall_class0 / (precision_class0 + recall_class0)\n",
        "print(true_pos_count, false_pos_count, false_neg_count, true_neg_count)\n",
        "\n",
        "precision_class1 = true_neg_count / (true_neg_count + false_neg_count)\n",
        "recall_class1 = true_neg_count / (true_neg_count + false_pos_count)\n",
        "f1_class1 = 2 * precision_class1 * recall_class1 / (precision_class1 + recall_class1)\n",
        "\n",
        "print('Primary task train accuracy: ' + str(prim_train_accuracy * 100))\n",
        "# print('Secondary task train accuracy: ' + str(sec_train_accuracy * 100))\n",
        "print('Precision: ', precision_class0)\n",
        "print('Recall: ', recall_class0)\n",
        "print('F1 score: ', f1_class0)\n",
        "print('Precision: ', precision_class1)\n",
        "print('Recall: ', recall_class1)\n",
        "print('F1 score: ', f1_class1)\n",
        "\n",
        "# print('Prim task test accuracy: ' + str(prim_train_accuracy * 100))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restore model\n",
            "INFO:tensorflow:Restoring parameters from ./sample_data/model.ckpt\n",
            "OK\n",
            "483 25 10 482\n",
            "Primary task train accuracy: 96.7\n",
            "Precision:  0.9507874015748031\n",
            "Recall:  0.9797160243407708\n",
            "F1 score:  0.9650349650349651\n",
            "Precision:  0.9796747967479674\n",
            "Recall:  0.9506903353057199\n",
            "F1 score:  0.964964964964965\n"
          ]
        }
      ]
    }
  ]
}